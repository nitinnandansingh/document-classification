{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TopicModeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP2LMMHU7vzw23GAjijehiv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bostelma/ATiML-Project/blob/master/TopicModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqbcLdFpT9hL",
        "colab_type": "text"
      },
      "source": [
        "# Topic Modeling\n",
        "This program tries classify books according to their genre based on topic modeling using LDA. To do so, first a number of topics gets exxtracted from the books in the test set. Using with which probability a topic is present in a certain book and which genre it is, we can train a classifier. This classifier can then be used to determine for books from the test set, which genre they might have."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwBehD-4VnvH",
        "colab_type": "text"
      },
      "source": [
        "# Program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBOjWE63WFqB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "e5d0378e-b865-4125-d166-5e70ed9d5933"
      },
      "source": [
        "from google.colab import drive\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from gensim import corpora\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import numpy as np\n",
        "import random\n",
        "from google.colab import output\n",
        "import nltk\n",
        "from gensim.models import Phrases\n",
        "nltk.download('wordnet')\n",
        "nltk.download('names')\n",
        "from nltk.corpus import names\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ufWUsseydV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_TOPICS = 5                # Number of topics to generate. Equals size of generated data\n",
        "USE_FIRST_N_WORDS = 2500      # Number of tokens to use for the topic generation\n",
        "SKIP_RANDOM_LITERARAY = False # For testing purposes ignore some literary books\n",
        "SKIP_PERCANTAGE = 0.8         # 80 Percent of all literary books will be skipped (randomly)\n",
        "SAVE_PREPARED = True          # Saves the completely processed tokens as a np array\n",
        "LOAD_PREPARED = False         # Load in the completely processed tokens to save time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga9JB2yJVsCO",
        "colab_type": "text"
      },
      "source": [
        "## Load in the already tokenized books"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxh4FogmIDiw",
        "colab_type": "text"
      },
      "source": [
        "You need to load in both csv files, the master file and the lemmatized content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMWB4VqsVX3r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "1cf61f05-6da8-4aab-9353-420fc24ddf75"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "pathLemmat = \"/content/drive/My Drive/ATIML/lemmatized_content.csv\"\n",
        "pathMaster = \"/content/drive/My Drive/ATIML/master996.csv\"\n",
        "\n",
        "lemmat = pd.read_csv( pathLemmat )\n",
        "master = pd.read_csv( pathMaster, encoding='windows-1252' , sep=';' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LemMi_hpvh45",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "541e9144-ae4e-4e94-b54d-bc507f3763f6"
      },
      "source": [
        "books = []\n",
        "genres = []\n",
        "\n",
        "count = 1\n",
        "\n",
        "for index, row in lemmat.iterrows():\n",
        "\n",
        "  genre = master['guten_genre'][ index ]\n",
        "\n",
        "  # Skip some literary books to improve the runtime for testing purposes\n",
        "  if SKIP_RANDOM_LITERARAY:\n",
        "    if genre == 'Literary':\n",
        "        val = random.random()\n",
        "        if val > 1 - SKIP_PERCANTAGE:\n",
        "          continue\n",
        "\n",
        "  # Get the tokens and remove words that are too short\n",
        "  tokens = []\n",
        "  try:\n",
        "    tokens = row['cleaned_Data_Content'].split(' ')\n",
        "    tokens = [ token for token in tokens if len(token) > 3 ]\n",
        "  except:\n",
        "    print('A book failed')\n",
        "\n",
        "  books.append( tokens )\n",
        "  genres.append( genre )\n",
        "\n",
        "  output.clear('status_text')\n",
        "  with output.use_tags('status_text'):\n",
        "    print( 'Books loaded: ' + str( count ))\n",
        "  count += 1\n",
        "\n",
        "genres = np.array( genres )\n",
        "books = np.array( books )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Books loaded: 996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk8lmu2PIKks",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bf0bb534-5a27-455b-e4b3-b86e7c0bacaf"
      },
      "source": [
        "# Additional data processing to optimize the topics generated\n",
        "\n",
        "commonWords = []\n",
        "\n",
        "with open( \"/content/drive/My Drive/ATIML/common.txt\" ) as common:\n",
        "  for word in common:\n",
        "      commonWords.append( word[:-1] )\n",
        "\n",
        "forbidden = [\n",
        "             'littl',\n",
        "             'think',\n",
        "             'befor',\n",
        "             'thought',\n",
        "             'never',\n",
        "             'thing',\n",
        "             'shall',\n",
        "             'someth',\n",
        "             'every',\n",
        "             'found',\n",
        "             'seemed',\n",
        "             'looked',\n",
        "             'turned',\n",
        "             'called'\n",
        "]\n",
        "\n",
        "count = 1\n",
        "\n",
        "tmp = []\n",
        "for b in books:\n",
        "\n",
        "  # Use only a subset of tokens\n",
        "  m = min( len(b), USE_FIRST_N_WORDS)\n",
        "\n",
        "  l = b[:m]\n",
        "  l = [ t for t in l if (t not in names.words('male.txt'))]\n",
        "  l = [ t for t in l if (t not in names.words('female.txt'))]\n",
        "  l = [ t for t in l if (t not in forbidden) ]\n",
        "  l = [ t for t in l if (t not in commonWords)]\n",
        " \n",
        "  tmp.append( l )\n",
        "\n",
        "  output.clear('status_text')\n",
        "  with output.use_tags('status_text'):\n",
        "    print( 'Books loaded: ' + str( count ))\n",
        "  count += 1\n",
        "\n",
        "books = tmp\n",
        "books = np.array( books )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Books loaded: 996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiljeB0eWSRv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "7eda49ee-eb6c-4796-ea89-af7f2a5220dd"
      },
      "source": [
        "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
        "bigram = Phrases(books, min_count=20)\n",
        "for idx in range(len(books)):\n",
        "    for token in bigram[books[idx]]:\n",
        "        if '_' in token:\n",
        "            # Token is a bigram, add to document.\n",
        "            books[idx].append(token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxPs6JhpLtuY",
        "colab_type": "text"
      },
      "source": [
        "Save and load funcitonality of the completely processed tokens to speed up the whole process in future runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emn4DJILLsze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = 'prepared_tokens.npy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxpt_GN-MQxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if SAVE_PREPARED:\n",
        "  with open(data_path, 'wb') as f:\n",
        "    np.save(f, genres)\n",
        "    np.save(f, books)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bgKJMF7NlK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if LOAD_PREPARED:\n",
        "  with open(data_path, 'rb') as f:\n",
        "    genres = np.load(f)\n",
        "    books = np.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ptMh4f4V00v",
        "colab_type": "text"
      },
      "source": [
        "## Create a train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9Tzoip7Vp8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUMBER_OF_SPLITS = 5\n",
        "TEST_SIZE = 1 / 3\n",
        "\n",
        "sss = StratifiedShuffleSplit(\n",
        "    n_splits=NUMBER_OF_SPLITS,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "splits = sss.split( books, genres )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6qRUBFMZ4Jm",
        "colab_type": "text"
      },
      "source": [
        "## Get the Topics and convert them to usable data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_llE6PMOe5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO add your train test split here, make sure to fill in \n",
        "\n",
        "train_index = []\n",
        "test_index = []\n",
        "\n",
        "for tr, te in splits:\n",
        "  train_index = tr\n",
        "  test_index = te\n",
        "  \n",
        "books_train, books_test = books[train_index], books[test_index]\n",
        "genres_train, genres_test = genres[train_index], genres[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhpgrgtCT_hd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "ae9a80dd-fa34-4567-e521-67f2461593e4"
      },
      "source": [
        "# Create the topics\n",
        "NUM_WORDS  = 6\n",
        "\n",
        "dictionary = corpora.Dictionary( books_train )\n",
        "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
        "corpus = [ dictionary.doc2bow( text ) for text in books_train ]\n",
        "\n",
        "# Set training parameters.\n",
        "num_topics = NUM_TOPICS\n",
        "chunksize = 2000\n",
        "passes = 20\n",
        "iterations = 400\n",
        "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
        "\n",
        "# Make a index to word dictionary.\n",
        "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
        "id2word = dictionary.id2token\n",
        "\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    chunksize=chunksize,\n",
        "    alpha='auto',\n",
        "    eta='auto',\n",
        "    iterations=iterations,\n",
        "    num_topics=num_topics,\n",
        "    passes=passes,\n",
        "    eval_every=eval_every\n",
        ")\n",
        "\n",
        "# Print out the topics\n",
        "topics = ldamodel.print_topics( num_words=NUM_WORDS )\n",
        "print(\"The following topics were generated:\")\n",
        "for topic in topics:\n",
        "  print( topic )\n",
        "\n",
        "# Process the books and get final training data\n",
        "X_train = []\n",
        "Y_train = genres_train # TODO do I have to preprocess it as well?\n",
        "\n",
        "for book in books_train:\n",
        "\n",
        "  # Get the topic weights\n",
        "  bow = dictionary.doc2bow( book )\n",
        "  topics = ldamodel.get_document_topics( bow )\n",
        "\n",
        "  # Convert the vector of dynamic length to\n",
        "  # constant length feature vector\n",
        "  x = [0] * NUM_TOPICS\n",
        "  for topic in topics:\n",
        "    x[topic[0]] = topic[1]\n",
        "  X_train.append(x)\n",
        "\n",
        "# Prepare our test data in the same way\n",
        "X_test = []\n",
        "Y_test = genres_test # TODO do I have to preprocess it as well?\n",
        "\n",
        "for book in books_test:\n",
        "\n",
        "  # Get the topic weights\n",
        "  bow = dictionary.doc2bow( book )\n",
        "  topics = ldamodel.get_document_topics(bow)\n",
        "\n",
        "  # Convert the vector of dynamic length to\n",
        "  # constant length feature vector\n",
        "  x = [0] * NUM_TOPICS\n",
        "  for topic in topics:\n",
        "    x[topic[0]] = topic[1]\n",
        "  X_test.append(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The following topics were generated:\n",
            "(0, '0.002*\"lord\" + 0.002*\"towards\" + 0.002*\"circumstance\" + 0.002*\"fortune\" + 0.002*\"stranger\" + 0.002*\"received\"')\n",
            "(1, '0.003*\"alice\" + 0.003*\"john\" + 0.002*\"lord\" + 0.002*\"hall\" + 0.002*\"james\" + 0.002*\"stone\"')\n",
            "(2, '0.002*\"david\" + 0.002*\"captain\" + 0.002*\"train\" + 0.002*\"aunt\" + 0.002*\"harry\" + 0.002*\"horse\"')\n",
            "(3, '0.004*\"john\" + 0.002*\"london\" + 0.002*\"marry\" + 0.002*\"english\" + 0.002*\"lord\" + 0.002*\"towards\"')\n",
            "(4, '0.003*\"horse\" + 0.003*\"captain\" + 0.002*\"boat\" + 0.002*\"hill\" + 0.002*\"ship\" + 0.002*\"wood\"')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}