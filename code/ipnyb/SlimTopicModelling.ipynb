{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SlimTopicModelling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqJV00LaDGpZ3JUMGMn8Os",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bostelma/ATiML-Project/blob/master/SlimTopicModelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4NJWkU0gF4F",
        "colab_type": "text"
      },
      "source": [
        "# Topic modeling\n",
        "This notebook lets you easily create the topics from a the data set and get the corresponding feature vectors. It makes use of the already preprocessed data that gets loaded in to spead up the process. The number of topics directly correlates to the size of the generated feature vector. However, note that the generation of topics includes randomness. Therefe, the topics and values are not fixed. They may change in each iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWl8vGq0ee_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_TOPICS = 10   # Number of topics and size of output vec\n",
        "PERCANTAGE = 0.2  # Percentages of Literary books to keep when\n",
        "                  # generating the topics, 1.0 means all"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSX5wXJFe37E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76RlrMoLeiNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genres = []\n",
        "books = []\n",
        "\n",
        "data_path = 'prepared_tokens.npy'\n",
        "\n",
        "with open(data_path, 'rb') as f:\n",
        "    genres = np.load(f, allow_pickle=True)\n",
        "    books = np.load(f, allow_pickle=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqNT2lUtguSn",
        "colab_type": "text"
      },
      "source": [
        "In this cell the test / train split happens. You have to adapt this part according to your code. The important thing is that the variabels: books_train, books_test, genres_train, and genres_test get filled with reasonable data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxnT0FycexQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUMBER_OF_SPLITS = 5\n",
        "TEST_SIZE = 1 / 3\n",
        "\n",
        "sss = StratifiedShuffleSplit(\n",
        "    n_splits=NUMBER_OF_SPLITS,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "splits = sss.split( books, genres )\n",
        "\n",
        "# TODO add your train test split here, make sure to fill in \n",
        "\n",
        "train_index = []\n",
        "test_index = []\n",
        "\n",
        "for tr, te in splits:\n",
        "  train_index = tr\n",
        "  test_index = te\n",
        "  \n",
        "books_train, books_test = books[train_index], books[test_index]\n",
        "genres_train, genres_test = genres[train_index], genres[test_index]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH3a87JzhAqv",
        "colab_type": "text"
      },
      "source": [
        "Here happens the actual processing stuff. At the end you have the four variables X_train, X_test, Y_train, and Y_test that you can use to train your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94uHcIBt9p8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arr = []\n",
        "\n",
        "for i in range( len(genres_train ) ):\n",
        "  if genres_train[i] == 'Literary':\n",
        "    val = random.random()\n",
        "    if val > PERCANTAGE:\n",
        "      continue\n",
        "  arr.append( books_train[i])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9BVn-Y0e0VT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the topics\n",
        "NUM_WORDS  = 4\n",
        "\n",
        "dictionary = corpora.Dictionary( arr )\n",
        "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
        "corpus = [ dictionary.doc2bow( text ) for text in arr ]\n",
        "\n",
        "# Set training parameters.\n",
        "num_topics = NUM_TOPICS\n",
        "chunksize = 2000\n",
        "passes = 20\n",
        "iterations = 400\n",
        "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
        "\n",
        "# Make a index to word dictionary.\n",
        "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
        "id2word = dictionary.id2token\n",
        "\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=id2word,\n",
        "    chunksize=chunksize,\n",
        "    alpha='auto',\n",
        "    eta='auto',\n",
        "    iterations=iterations,\n",
        "    num_topics=num_topics,\n",
        "    passes=passes,\n",
        "    eval_every=eval_every\n",
        ")\n",
        "\n",
        "# Print out the topics\n",
        "topics = ldamodel.print_topics( num_words=NUM_WORDS )\n",
        "print(\"The following topics were generated:\")\n",
        "for topic in topics:\n",
        "  print( topic )\n",
        "\n",
        "# Process the books and get final training data\n",
        "X_train = []\n",
        "Y_train = genres_train # TODO do I have to preprocess it as well?\n",
        "\n",
        "for book in books_train:\n",
        "\n",
        "  # Get the topic weights\n",
        "  bow = dictionary.doc2bow( book )\n",
        "  topics = ldamodel.get_document_topics( bow )\n",
        "\n",
        "  # Convert the vector of dynamic length to\n",
        "  # constant length feature vector\n",
        "  x = [0] * NUM_TOPICS\n",
        "  for topic in topics:\n",
        "    x[topic[0]] = topic[1]\n",
        "  X_train.append(x)\n",
        "\n",
        "# Prepare our test data in the same way\n",
        "X_test = []\n",
        "Y_test = genres_test # TODO do I have to preprocess it as well?\n",
        "\n",
        "for book in books_test:\n",
        "\n",
        "  # Get the topic weights\n",
        "  bow = dictionary.doc2bow( book )\n",
        "  topics = ldamodel.get_document_topics(bow)\n",
        "\n",
        "  # Convert the vector of dynamic length to\n",
        "  # constant length feature vector\n",
        "  x = [0] * NUM_TOPICS\n",
        "  for topic in topics:\n",
        "    x[topic[0]] = topic[1]\n",
        "  X_test.append(x)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}